var documenterSearchIndex = {"docs":
[{"location":"api_usage/#API-components","page":"API Usage","title":"API components","text":"","category":"section"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC API consist of three main components:","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"Backend selection: e.g. JACC.set_backend(\"CUDA\") and JACC.@init_backend\nMemory models: e.g. JACC.array{T}(), JACC.zeros, JACC.ones, JACC.shared, JACC.Multi\nKernel launching: e.g. JACC.parallel_for, JACC.parallel_reduce","category":"page"},{"location":"api_usage/#Backend-selection","page":"API Usage","title":"Backend selection","text":"","category":"section"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC.set_backend: allows selecting the runtime back end on CPU: Threads (default) and GPU: CUDA, AMDGPU, oneAPI. Uses Preferences.jl and stores the selected back end in a LocalPreferences.jl file if JACC.jl is a project dependency. Use JACC.set_backend prior to running any code targeting a particular back end.","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"Example:","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"using JACC\nJACC.set_backend(\"CUDA\")","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"danger: Danger\nset_backend will polute your project's Project.toml adding the selected backend package. Beware of committing this change (e.g. during development). ","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"warning: Warning\nThis step might take a while the very first time downloading all back end dependencies. ","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"tip: Tip for CUDA\nCUDA.jl uses its own prebuilt CUDA stack by default, please refer to CUDA.jl docs if wanting to use a local CUDA installation to set up LocalPreferences.toml. ","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"tip: Tip for AMDGPU\nAMDGPU.jl relies on standard rocm installation under /opt/rocm, for non-standard locations set the environment variable ROCM_PATH, see docs.","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC.@init_backend: initializes the selected back end automatically from LocalPreferences.toml from JACC.set_backend. JACC.@init_backend should be used in your code before using any JACC.jl functionality. Recent improvements have made this process more seamless.","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"Example:","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"using JACC\nJACC.@init_backend","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"tip: Tip\nAlways use JACC.@init_backend right after import JACC or using JACC for portable back end agnostic code.","category":"page"},{"location":"api_usage/#Memory-models","page":"API Usage","title":"Memory models","text":"","category":"section"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC.array{T}(): create a new array on the device with the specified type and size.\nJACC.zeros: create a new array on the device filled with zeros.\nJACC.ones: create a new array on the device filled with ones.","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"Advanced memory:","category":"page"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC.shared: exploit fast-access cache memory on device. Use it inside kernel functions. Please see the paper Valero-Lara IEEE HPEC 2024 for more details.\nJACC.Multi: allows the programmability of multiple-GPU devices on a single node without the need of MPI. Please see the paper Valero-Lara et al. IEEE eScience 2025 \nJACC.@atomic: create an atomic variable on the device for safe concurrent access. Wraps @atomic from the supported Atomix.jl package in the JuliaGPU ecosystem.","category":"page"},{"location":"api_usage/#Kernel-launching","page":"API Usage","title":"Kernel launching","text":"","category":"section"},{"location":"api_usage/","page":"API Usage","title":"API Usage","text":"JACC.parallel_for: launch a parallel for loop (each i index is independent).\nJACC.parallel_reduce: launch a parallel reduce operation.","category":"page"},{"location":"#JACC-Introduction","page":"Welcome","title":"JACC Introduction","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"JACC.jl is a Julia package for performance portable CPU/GPU kernels using metaprogramming on top of existing Julia back ends. It enables users to write a single code based on an array memory model and parallel_for and parallel_reduce functions that can run on both CPUs and GPUs without needing to rewrite the code for each platform.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"While the JuliaGPU ecosystem provides powerful tools for GPU programming, JACC leverages these capabilities to simplify the process of writing portable code that can run on both CPUs and GPUs without modification. Hence JACC is complementary to existing solutions in Julia. Users do not need to worry about the underlying hardware and software, or GPU low-level programming concepts. ","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"JACC overview:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"(Image: JACC Architecture)","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Resources:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"For a broader understanding of JACC's design principles and goals, please refer to the JACC paper at SC-W 2024 - open version available here.\nFor a full example using JACC, see the Gray-Scott code and \"Julia for HPC\" Tutorial material","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"why: Why JACC?\nJACC's default basic APIs enable minimal effort to access CPU and GPU parallel capabilities to accelerate codes and let users focus on their algorithms and science.","category":"page"},{"location":"#Why-Julia?","page":"Welcome","title":"Why Julia?","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Julia enables scientists and engineers to write code more quickly and to focus on their science or technical domain. ","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Julia combines high-level/low-level capabilities for easy to read and write code due to its expressive mathematical syntax (e.g. arrays, GPU programming, parallelization). \nJulia's just-in-time (JIT) compilation based on LLVM allows for speeds comparable to C, C++ or Fortran.\nJulia's unified package manager, Pkg.jl, simplifies the process of managing dependencies.\nJulia's rich ecosystem includes a wide range of libraries and tools for scientific computing, data analysis, and machine learning.","category":"page"},{"location":"#Supported-backends","page":"Welcome","title":"Supported backends","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"CPU (default):\nThreads: multi-threading on CPU using Julia's built-in threading capabilities.\nGPU from JuliaGPU:\nCUDA: NVIDIA GPUs using the CUDA.jl package.\nAMDGPU: AMD GPUs using the AMDGPU.jl package.\noneAPI: Intel GPUs using the oneAPI.jl package (Experimental).","category":"page"},{"location":"#Installation","page":"Welcome","title":"Installation","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"JACC is a registed Julia package. Install JACC using Julia's Pkg.jl capabilities for managing packages.  e.g.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"pkg> add JACC","category":"page"},{"location":"#Quick-start-filling-and-reducing-an-array","page":"Welcome","title":"Quick start - filling and reducing an array","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Prerequisite: ","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Julia 1.11 or later","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"To run an example from scratch","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Install JACC.jl","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"julia -e 'using Pkg; Pkg.add(\"JACC\")'","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Copy this file to your local machine and save it as jacc-saxpy.jl:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"import JACC\nJACC.@init_backend\n\nfunction axpy(i, alpha, x, y)\n    @inbounds x[i] += alpha * y[i]\nend\n\nN = 100_000\nalpha = 2.0\nx = JACC.zeros(Float64, N)\ny = JACC.array(fill(Float32(5), N))\nJACC.parallel_for(N, axpy, alpha, x, y)\na = JACC.parallel_reduce(x)\nprintln(\"Result: \", a)","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Run the example using the default Threads back end with 4 threads:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"julia -t 4 jacc-saxpy.jl","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Switch to another back end, e.g. assuming access to NVIDIA GPU and that CUDA is installed and configured:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"julia -e 'using JACC; JACC.set_backend(\"CUDA\")'","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"note: Note\nThis step might take a while the first time downloading all CUDA.jl dependencies. After installation please refer to CUDA.jl docs if wanting to use a local CUDA installation.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Run the example again, but this time the same code will run on the GPU:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"julia jacc-saxpy.jl","category":"page"},{"location":"acknowledgment/#Acknowledgements","page":"Acknowledgments","title":"Acknowledgements","text":"","category":"section"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"JACC is funded by the US Department of Energy Advanced Scientific Computing Research (ASCR) projects:","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"S4PST and PESO as part of the Next Generation of Scientific Software Technologies (NGSST) ASCR Program. \nNGSST sponsors the Consortium for the Advancement of Scientific Software, CASS\nASCR Competitive Portfolios for Advanced Scientific Computing Research, MAGMA/Fairbanks","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"Former sponsors:","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"ASCR Bluestone X-Stack\nThe Exascale Computing Project - PROTEAS-TUNE ","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"JACC would not be possible without the contributions of the Julia language and the JuliaGPU community and the amazing GPU work of the CUDA.jl, AMDGPU.jl, oneAPI.jl and Metal.jl backend developers.","category":"page"},{"location":"acknowledgment/#Citing","page":"Acknowledgments","title":"Citing","text":"","category":"section"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"Much of JACC is motivated by the Julia desire to make high-performance computing more accessible to a broader range of users. If you use JACC in your research or projects, we would appreciate it if you could cite our paper from SC24-WAACPD, open version available here.","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"bib entry:","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"@INPROCEEDINGS{JACC,\n  author={Valero-Lara, Pedro and Godoy, William F and Mankad, Het and Teranishi, Keita and Vetter, Jeffrey S and Blaschke, Johannes and Schanen, Michel},\n  booktitle={Proceedings of the SC '24 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},\n  title={{JACC: Leveraging HPC Meta-Programming and Performance Portability with the Just-in-Time and LLVM-based Julia Language}},\n  year={2024},\n  volume={},\n  number={},\n  pages={},\n  doi={10.1109/SCW63240.2024.00245}\n}","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"Other papers that contributed to JACC's exploratory research include:","category":"page"},{"location":"acknowledgment/","page":"Acknowledgments","title":"Acknowledgments","text":"JACC shared at IEEE HPEC\nJACC Multi, accepted at IEEE eScience","category":"page"}]
}
